{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b5cfd1",
   "metadata": {},
   "source": [
    "# A Framawork for Input Reconstruction by Convex Optimization - Notes\n",
    "M.M., U.H\n",
    "## Motivation\n",
    "The input reconstruction problem has previously been solved by filtering methodssuch as [augmented Kalman Fitlers](https://www.sciencedirect.com/science/article/pii/S0888327022001480) and [SISE](https://www.researchgate.net/profile/Bart-De-Moor/publication/222567614_Unbiased_minimum-variance_input_and_state_estimation_for_linear_discrete-time_systems/links/5aa245b4a6fdcc22e2d2e9a2/Unbiased-minimum-variance-input-and-state-estimation-for-linear-discrete-time-systems.pdf) estimators.\n",
    "\n",
    "Formulating the batch input-reconstruction problem as a convex optimization problem comes at the cost of computational efficiency. Convex optimization has traditionally been considered computationally expensive and has thus had limited use in industry. However, such restrictions are no longer justified ([Boyd, 2010](https://web.stanford.edu/~boyd/papers/pdf/rt_cvx_sig_proc.pdf)). Computational costs for modestly sized convex optimization problems is not a relevant issue as the computational power has increased, industrial PC are replacing/complementing PLC controllers in plants, and optimization software are improving. In our opinion, the added benefits of posing the problem as a convex optimization problem outweigh the downsides.\n",
    "\n",
    "Some added benefits of posingn the problem as a convex program are:\n",
    "- Input models and restrictions can easily be included in the optimization problem\n",
    "- Non-uniform sampling intervals can easily be handled\n",
    "- More intuitive interpolation scemes possible (?). ZOH used in filtering not always realistic...\n",
    "- Constraints ensuring that result are phycally justified can easily be included (e.g. satisfies mass&energy balances, non-negative signals etc.)\n",
    "\n",
    "Some chalenges posing the input reconstruction problem as a convex program:\n",
    "- Real-time implementations are stil challenging for high sampling rates\n",
    "\n",
    "## System model\n",
    "Consider the discrete-time state-space represetnation of a system\n",
    "$$\n",
    "x(k+1) = Ax(k) + Bu(k), \\\\\n",
    "y(k) = Cx(k) + v(k),\n",
    "$$\n",
    "where $x(k)\\in\\mathbb{R}^n$ are the system states, $u(k)\\in\\mathbb{R}^m$ are (unknown) input signals, $y(k)\\in\\mathbb{R}^l$ measurements, and $v(k)\\in\\mathbb{R}^l$ measurement noise. Repeated substitution of the state equation into the measurement equation gives\n",
    "$$y = \\mathcal{O}x(0) + \\Gamma u + v, $$\n",
    "where\n",
    "$$\n",
    "y = \\begin{bmatrix}\n",
    "y(0)\\\\ \\vdots \\\\ y(N-1)\n",
    "\\end{bmatrix},\n",
    "~u = \\begin{bmatrix}\n",
    "u(0)\\\\ \\vdots \\\\ u(N-1)\n",
    "\\end{bmatrix},\n",
    "v = \\begin{bmatrix}\n",
    "v(0)\\\\ \\vdots \\\\ v(N-1)\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\Gamma = \\begin{bmatrix}\n",
    "D   &0  &0  &\\cdots &0\\\\\n",
    "        CB  &D  &0  &\\cdots &0\\\\\n",
    "        CAB &CB &D  & &0\\\\\n",
    "        \\vdots  & &\\ddots &\\ddots &\\\\\n",
    "        CA^{N-2} &CA^{N-3} &\\cdots &CB &D\n",
    "\\end{bmatrix}\n",
    ",~\\text{and}~\n",
    "\\mathcal{O} = \n",
    "\\begin{bmatrix}\n",
    "    C\\\\CA\\\\CA^2\\\\ \\vdots \\\\ CA^{N-1}.\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For simplicity, assume $x(0)=0$. $x(0)$ can be included as an unknown variable if needed. The input reconstruction problem can be formulated as the following least-squares regeression problem\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "    &\\underset{u\\in\\mathbb{R}^{Nm},~v\\in\\mathbb{R}^{Nl}}{\\text{minimize}} \\quad L(u) + \\sum_{k=0}^{N-1} v(k)^TR^{-1}v(k)\\\\\n",
    "    &\\text{subject to}\\\\\n",
    "    &v = y - \\Gamma u\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "where $L(u)$ is a regularization term, preferrably design such that $L(u)\\in[0,~1]$. Note that without the regularization term $L(u)$ the system might be ill-posed of $m>l$ or $\\Gamma$ is rank deficient (not full rank). In the following sections, various ways of regularizing the problem are proposed, all resulting in convex optimization problems.\n",
    "\n",
    "For known \n",
    "\n",
    "\n",
    "# Problem formulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319df565",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "For square-summable signals $u_k$, a natural choise of regularization is the $\\ell_2$ norm. Setting $L(u) = \\| u \\|_2^2$ gives the optimization problem\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "    &\\underset{u\\in\\mathbb{R}^{Nm},~v\\in\\mathbb{R}^{Nl}}{\\text{minimize}} \\quad \\lambda\\|u\\|_2^2 + \\sum_{k=0}^{N-1} v(k)^TR^{-1}v(k)\\\\\n",
    "    &\\text{subject to}\\\\\n",
    "    &v = y - \\Gamma u\n",
    "    \\end{align},\n",
    "$$\n",
    "\n",
    "where $\\lambda >0$ is a regularization parameter determining the trade-off between regularization and outpur error.\n",
    "\n",
    "When $u$ is a stochastic signal with known covariance $Q = \\mathrm{E}[u(k)^Tu(k)]$, the optimal weightingn is obtained by \n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "    &\\underset{u\\in\\mathbb{R}^{Nm},~v\\in\\mathbb{R}^{Nl}}{\\text{minimize}} \\sum_{k=0}^{N-1} u(k)^TQ^{-1}u(k) + \\sum_{k=0}^{N-1} v(k)^TR^{-1}v(k)\\\\\n",
    "    &\\text{subject to}\\\\\n",
    "    &v = y - \\Gamma u.\n",
    "    \\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "140e0b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47eb9c9",
   "metadata": {},
   "source": [
    "### LASSO regression\n",
    "Same as LASSO but with $L(u) = \\| u \\|_1$. This might be a useful formulation when $u(k)$ is sparse, e.g., outliers or impulse-like disturbances.\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "    &\\underset{u\\in\\mathbb{R}^{Nm},~v\\in\\mathbb{R}^{Nl}}{\\text{minimize}} \\quad \\lambda\\|u\\|_1 + \\sum_{k=0}^{N-1} v(k)^TR^{-1}v(k)\\\\\n",
    "    &\\text{subject to}\\\\\n",
    "    &v = y - \\Gamma u\n",
    "    \\end{align},\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd377c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c69a3",
   "metadata": {},
   "source": [
    "### Sparse frequency-domain formulation\n",
    "Signals often have sparse frequency-domain representations. Expanding the unknown input signal $u$ using the discrete Fourier transform (DFT)\n",
    "\n",
    "$$\n",
    "    u(k) = \\frac{1}{N}\\sum_{n=0}^{N-1} U_n \\mathrm{e}^{\\mathrm{i}2\\pi kn/N},~k=0,1,...,N-1,\n",
    "$$\n",
    "\n",
    "where $U_n$ is the $n$th Fourier coefficient providing information on the amplitude and phase of the signal at frequencies $\\omega_n = 2\\pi n/N$ for $k=0,1,...,N-1$. The DFT representation of $u$ can be written in matrix form\n",
    "\n",
    "$$\n",
    "    u = \\Phi U\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "    \\Phi =\n",
    "    \\frac{1}{N}\n",
    "    \\begin{bmatrix}\n",
    "    1 &1 &1 &\\cdots &1\\\\\n",
    "          1 &\\mathrm{e}^{\\mathrm{i}2\\pi/N} &\\mathrm{e}^{\\mathrm{i}4\\pi/N} &\\cdots &\\mathrm{e}^{\\mathrm{i}2(N-1)\\pi/N}\\\\\n",
    "          1 &\\mathrm{e}^{\\mathrm{i}4\\pi/N} &\\mathrm{e}^{\\mathrm{i}8\\pi/N} &\\cdots &\\mathrm{e}^{\\mathrm{i}4(N-1)\\pi/N}\\\\\n",
    "          \\\\\n",
    "          1 &\\mathrm{e}^{\\mathrm{i}2(N-1)\\pi/N} &\\mathrm{e}^{\\mathrm{i}4(N-1)\\pi/N} &\\cdots &\\mathrm{e}^{\\mathrm{i}2(N-1)^2\\pi/N}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For real signals $u$, the Fourier coefficient appear in complex conjugated pairs such that $U_k = U^\\ast_{N-k}$, $k=1,2,...,N-1$, and $U(0)\\in\\mathbb{R}$. \n",
    "\n",
    "This gives the convex optimization problem\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\underset{U\\in\\mathbb{C}^{Nm},v\\in\\mathbb{R}^{Nl}}{\\text{minimize}} \\quad& \\lambda\\| U \\|_1 + \\sum_{k=0}^{N-1} v(k)^\\top R^{-1} v(k) \\\\\n",
    "    \\text{subject to} \\quad& v = y - \\Gamma\\Phi U\\\\\n",
    "    \\quad& U_k = U^\\ast_{N-k},~k=1,2,...,N-1.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The input estimate is $u = \\Phi U$. It is well-known that the $\\ell_1$ norm induces sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0921e0",
   "metadata": {},
   "source": [
    "### Trend filtering formulation\n",
    "Hodrick-Prescott (H-P) trend filtering and $\\ell_1$ trend filtering can be applied to induce smooth estimnates or piecewise estimates respectively. The H-P trend-filtering method uses\n",
    "\n",
    "$$\n",
    "    L_{\\mathrm{H-P}}(u) = \\sum_{k=1}^{N-2} ( u(k-1) - 2u(k) + u(k+1)  )^2 = \\| \\Delta_2 u\\|_2^2,\n",
    "$$\n",
    "\n",
    "and the $\\ell_1$ trend fitlering\n",
    "\n",
    "$$\n",
    "    L_{\\ell_1}(u) = \\sum_{k=1}^{N-2}  |u(k-1) - 2u(k) + u(k+1)  | = \\| \\Delta_2 u\\|_1,\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e957b69d",
   "metadata": {},
   "source": [
    "### Cubic spline formulation\n",
    "Cubic splines function defined by piecewise polynoimals used for smoothing. The cubic spline is a piecewise interpolation model fits a cubic polynomial to each pais or data ([Cubic Spline Intro](https://towardsdatascience.com/cubic-splines-the-ultimate-regression-model-bd51a9cf396d)). To ensure that the interpolation is smooth, teh 1st and 2nd derivatives are set to be equal at every point where two polynomials meet.\n",
    "\n",
    "#### Mathematics of a cubic spline\n",
    "##### Single knot\n",
    "Consider a set of $N$ datapoints $y(t_0), y(t_1),...,y(t_{N-1})$ on the interval $t_k \\in [a, b]$. Partitioning the interval $[a,~b]$ into two parts with a so called knot point $c \\in (a,~b)$, a cubic spline consisting of two piecewise cubic polynomials \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  f_1(t)&=a_{11}t^3 + a_{12}t^2 + a_{13}t + a_{14},    & a \\leq t < c,\\\\\n",
    "  f_2(t)&=a_{21}t^3 + a_{22}t^2 + a_{23}t + a_{24},    & c \\leq t \\leq b.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "that interpolates the data as\n",
    "$$\n",
    "y(t) =\n",
    "\\begin{cases}\n",
    "f_1(t), &a \\leq t < c,\\\\\n",
    "f_2(t), &c \\leq t < b.\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The regression problem consists of determining the parameters $a_{ij}$ such that the least-squares cost is minimized between knots and ensuring that the transition between knots is smooth. Smoothness can be enforces by the following conditions\n",
    "* Start and end conditions: $f_1(a) = y_\\mathrm{start}$, $f_2(b) = y_\\mathrm{end}$\n",
    "* Continuity as knot point: $f_1(c) = f_2(c)$,\n",
    "* Differentiability as knot point: $\\frac{\\mathrm{d}}{\\mathrm{d}t}f_1(c) = \\frac{\\mathrm{d}}{\\mathrm{d}t}f_2(c)$,\n",
    "* Smoothness as knot point: $\\frac{\\mathrm{d^2}}{\\mathrm{d}t^2}f_1(c) = \\frac{\\mathrm{d}^2}{\\mathrm{d}t^2}f_2(c)$.\n",
    "\n",
    "Note that $f_2^{(n)}(c) = f_2^{(n)}(c)$ is automatically satisfied for higher-order derivatives $n>2$ since $f_i^{(n)}(c) = 0$ for cubic splines. Ecaluating the first and second derivatives at point $c$ gives the conditions\n",
    "$$\n",
    "\\begin{align}\n",
    "    &3(a_{11} - a_{21})c^2 + 2(a_{12}-a_{22})c + (a_{13} - a_{23}) = 0 \\\\\n",
    "    &6(a_{11} - a_{21})c + 2(a_{12} - a_{22}) = 0.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "##### Multiple knots\n",
    "In general, on might want to divide the domain into $K \\leq N-1$ knot points such that the knots $c_i$ are placed at datapoints: $c_k = k$ for $k\\in[0,~N-1]$. Defining matrices\n",
    "$$\n",
    "\\begin{align}\n",
    "    A &=\n",
    "    \\begin{bmatrix}\n",
    "    a_1 &a_2 &\\cdots &a_K\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "    a_{11} &a_{21} &\\cdots &a_{K+1,1}\\\\\n",
    "    a_{12} &a_{22} &\\cdots &a_{K+1,2}\\\\\n",
    "    a_{13} &a_{23} &\\cdots &a_{K+1,3}\\\\\n",
    "    a_{14} &a_{24} &\\cdots &a_{K+1,4}\n",
    "    \\end{bmatrix},\\\\\n",
    "    %\n",
    "    \\Phi &=\n",
    "    \\begin{bmatrix}\n",
    "    \\varphi(0)^T\\\\\n",
    "    \\varphi(1)^T\\\\\n",
    "    \\vdots\\\\\n",
    "    \\varphi(N-1)^T\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "    t_0^3 &t_0^2 &t_0 &1\\\\\n",
    "    t_1^3 &t_1^2 &t_1 &1\\\\\n",
    "    \\vdots &\\vdots &\\vdots &\\vdots\\\\\n",
    "    t_{N-1}^3 &t_{N-1}^2 &t_{N-1} &1\\\\\n",
    "    \\end{bmatrix},\\\\\n",
    "    %\n",
    "    F &= \n",
    "    \\begin{bmatrix}\n",
    "    f_1 &f_2 &\\cdots &f_{K+1}\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "    f_1(t_0) &f_2(t_0) &\\cdots &f_{K+1}(t_0)\\\\\n",
    "    f_1(t_1) &f_2(t_1) &\\cdots &f_{K+1}(t_1)\\\\\n",
    "    \\vdots &\\vdots &       &\\vdots\\\\\n",
    "    f_1(t_{N-1}) &f_2(t_{N-1}) &\\cdots &f_{K+1}(t_{N-1})\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "we have that\n",
    "$$\n",
    "F = \\Phi A,~\\text{and}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "f_i(t_k) = \\varphi(t_k)^Ta_i, \\quad i=1,...,K, k=0,1...,N-1.\n",
    "$$\n",
    "\n",
    "The interpolation scheme is\n",
    "$$\n",
    "\\hat{y}(k) = \n",
    "\\begin{cases}\n",
    "f_1(t_k), & k < c_1,\\\\\n",
    "f_2(t_k), &c_1 \\leq k < c_2.\\\\\n",
    "\\vdots &\\vdots\\\\\n",
    "f_{K}(t_k), &c_{K-1} \\leq k < c_{K}\\\\\n",
    "f_{K+1}(t_k), &c_{K} \\leq k.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Resulting in the optimization problem\n",
    "$$\n",
    "    \\begin{align}\n",
    "    &\\underset{\\hat{y},~a_i}{\\text{minimize}} \\quad \\|  y - \\hat{y} \\|_2^2\\\\\n",
    "    &\\text{subject to}\\\\\n",
    "    \\text{ Piece-wise regression}\n",
    "    &\\begin{cases}\n",
    "        f_i(t_k) = \\varphi(t_k)^Ta_i, &i=1,...,K,~k=0,1...,N-1 \\\\\n",
    "    \\end{cases}\\\\\n",
    "    %\n",
    "    \\text{Interpolation }\n",
    "    &\\begin{cases}\n",
    "        \\hat{y}(t_k) = f_1(t_k), &0 < k <c_1\\\\\n",
    "        \\hat{y}(t_k) = f_i(t_k), &c_{i-1} \\leq k < c_{i},~i=2,...,K\\\\\n",
    "        \\hat{y}(t_k) = f_{K+1}(t_k), &c_K \\leq k < N-1.\\\\\n",
    "    \\end{cases}\\\\\n",
    "    %\n",
    "    \\text{Knot-point conditions }\n",
    "    &\\begin{cases}\n",
    "        0 = 3(a_{i1} - a_{i+1,1})c_i^2 + 2(a_{i2}-a_{i+1,2})c_i + (a_{i3} - a_{23}), &i=1,..,K+1, \\\\\n",
    "        0 = 6(a_{i1} - a_{i+1,1})c_i + 2(a_{i2} - a_{i+1,2}), &i=1,..,K+1,\n",
    "    \\end{cases}\\\\\n",
    "    %\n",
    "    \\text{Start and end conditions }\n",
    "    &\\begin{cases}\n",
    "        f_1(t_0) = y_{\\mathrm{start}},\\\\\n",
    "        f_{K+1}(t_{N-1}) = y_{\\mathrm{end}}.\n",
    "    \\end{cases}\n",
    "    \\end{align},\n",
    "$$\n",
    "\n",
    "Note that although this might look complicated, it can be rewritten as a standard convex optimization problem (not same $A,f,b$ as before)\n",
    "$$\n",
    "    \\begin{align}\n",
    "    &\\underset{x}{\\text{minimize}} \\quad f(x)\\\\\n",
    "    &\\text{subject to}\\\\\n",
    "    &Ax - b = 0.\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "<font color=\"red\">Verify this! Might have some even simpler solution since only equality constraint and quadratic objective. Solvable using Lagrange multipliers (KKT)?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a628212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4964b4",
   "metadata": {},
   "source": [
    "## Support-vector formulation\n",
    "### Least-square support-vector regression\n",
    "### The $\\varepsilon$-insensitivity loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347b146",
   "metadata": {},
   "source": [
    "# Real-time convex optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14bd39",
   "metadata": {},
   "source": [
    "## Filtering implementation (WIP)\n",
    "Note that the SISE problem can (supposedly) besolved on a sample-by-sample basis by posing it as a convex optimization problem, cf. the state-estimation problem ([Boyd, 2010](https://web.stanford.edu/~boyd/papers/pdf/rt_cvx_sig_proc.pdf)). \n",
    "\n",
    "Consider the system model with known measurement noise covariance $R$. The CVX input-and-state-estimation problem would be done in the folliwing steps\n",
    "\n",
    "0. Initialization: $\\hat{x}_{0,-1}$, $R, P = (I-KC)( APA^T + Q ), K = PC^T(CPC^T + v)^{-1}$\n",
    "1. Optimization:\n",
    "$$\n",
    "    \\begin{align}\n",
    "    &\\underset{x_k,u_{k|k-1},v_k}{\\text{minimize}} \\quad L(u_k) + v_k^T R v_k + ( x_k - \\hat{x}_{k|k-1} )^TP^{-1}( x_k - \\hat{x}_{k|k-1} )\\\\\n",
    "    &\\text{subject to}\\\\\n",
    "    &\\hat{x}_{k|k-1} = A\\hat{x}_{k-1|k-1} + Bu_{k|k-1}\\\\\n",
    "    &y_k = Cx_k + v_k\n",
    "    \\end{align}\n",
    "$$\n",
    "2. Prediction: $\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K(y - C\\hat{x}_{k|k-1})$\n",
    "\n",
    "For known inputs $u_k$ this formulation implements a **filtering Kalman filter**. Note that there are several reasons to believe that this method is flawed.\n",
    "* The state error covariance $P$ is precomputed in (0.) is (?) dependent on input estimates $u_k$.\n",
    "* In (1.) the outputs $y_k$ are leanearily dependent on both $x_k, u_k$ and $v_k$. Thus, without additional contraints on $u_k$ the output error $v_k$ can either be attributed to $x_k$, $u_k$, or $v_k$. Hence, the term $L(u)$ was included. Determining $L(u)$ for completely unknown signals might be tricky! \n",
    "* The input update is missing from (2.), i.e., measurements are not used to update $u$ ?\n",
    "\n",
    "These flaws illustrates the main challenges with SISE estimation. Ideas from [(Gillijns, 2007)](https://www.researchgate.net/profile/Bart-De-Moor/publication/222567614_Unbiased_minimum-variance_input_and_state_estimation_for_linear_discrete-time_systems/links/5aa245b4a6fdcc22e2d2e9a2/Unbiased-minimum-variance-input-and-state-estimation-for-linear-discrete-time-systems.pdf) could be incorporated here.\n",
    "\n",
    "## Recursive least squares\n",
    "The input reconstructoin problem can be implemented as a [recursive least-squares](https://en.wikipedia.org/wiki/Recursive_least_squares_filter) problem with exponential forgetting.\n",
    "\n",
    "## Moving-horizon estimation\n",
    "A moving-horizon implementation of the input reconstruction problem would be a nice contribution to control & estimation theory. \n",
    "\n",
    "## Real-time CVX optimization\n",
    "For example, Boyd has presented real-time CVX frameworks on seminars. We should follow this progress.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f9197",
   "metadata": {},
   "source": [
    "# Publication plan\n",
    "1. IFAC CAMS 2024 Conference Paper: Reconstruction of smooth unknown input signals\n",
    "    - Trend filtering\n",
    "    - Cubic splines\n",
    "2. Conference Paper TBD: Input reconstruction by sparse optimization\n",
    "    - Sparse DFT representation\n",
    "    - Support-vector formaulation\n",
    "3. Journal paper TBD: A convex-optimization framework for input and state estimation (CVX-SISE)\n",
    "    - Framework + source code\n",
    "    - Application\n",
    "4. Journal paper TBD: Real-time implementation of CVX input reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4961b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
